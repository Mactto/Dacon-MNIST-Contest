{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from numpy import expand_dims\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./dataset/train.csv')\n",
    "test = pd.read_csv('./dataset/test.csv')\n",
    "sub = pd.read_csv('./dataset/submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>digit</th>\n",
       "      <th>letter</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>...</th>\n",
       "      <th>774</th>\n",
       "      <th>775</th>\n",
       "      <th>776</th>\n",
       "      <th>777</th>\n",
       "      <th>778</th>\n",
       "      <th>779</th>\n",
       "      <th>780</th>\n",
       "      <th>781</th>\n",
       "      <th>782</th>\n",
       "      <th>783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>L</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>B</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>L</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>D</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2043</th>\n",
       "      <td>2044</td>\n",
       "      <td>6</td>\n",
       "      <td>V</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2044</th>\n",
       "      <td>2045</td>\n",
       "      <td>1</td>\n",
       "      <td>L</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2045</th>\n",
       "      <td>2046</td>\n",
       "      <td>9</td>\n",
       "      <td>A</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2046</th>\n",
       "      <td>2047</td>\n",
       "      <td>0</td>\n",
       "      <td>Z</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2047</th>\n",
       "      <td>2048</td>\n",
       "      <td>5</td>\n",
       "      <td>Z</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2048 rows × 787 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  digit letter  0  1  2  3  4  5  6  ...  774  775  776  777  778  \\\n",
       "0        1      5      L  1  1  1  4  3  0  0  ...    2    1    0    1    2   \n",
       "1        2      0      B  0  4  0  0  4  1  1  ...    0    3    0    1    4   \n",
       "2        3      4      L  1  1  2  2  1  1  1  ...    3    3    3    0    2   \n",
       "3        4      9      D  1  2  0  2  0  4  0  ...    3    3    2    0    1   \n",
       "4        5      6      A  3  0  2  4  0  3  0  ...    4    4    3    2    1   \n",
       "...    ...    ...    ... .. .. .. .. .. .. ..  ...  ...  ...  ...  ...  ...   \n",
       "2043  2044      6      V  2  4  3  4  2  4  4  ...    0    2    2    0    0   \n",
       "2044  2045      1      L  3  2  2  1  1  4  0  ...    2    3    4    2    1   \n",
       "2045  2046      9      A  4  0  4  0  2  4  4  ...    2    3    1    1    3   \n",
       "2046  2047      0      Z  2  3  3  0  3  0  4  ...    2    3    1    1    0   \n",
       "2047  2048      5      Z  4  2  2  1  3  0  0  ...    4    2    4    0    4   \n",
       "\n",
       "      779  780  781  782  783  \n",
       "0       4    4    4    3    4  \n",
       "1       1    4    2    1    2  \n",
       "2       0    3    0    2    2  \n",
       "3       4    0    0    1    1  \n",
       "4       3    4    3    1    2  \n",
       "...   ...  ...  ...  ...  ...  \n",
       "2043    1    3    1    4    0  \n",
       "2044    2    3    4    1    1  \n",
       "2045    4    2    2    0    0  \n",
       "2046    4    1    4    3    1  \n",
       "2047    3    2    4    3    4  \n",
       "\n",
       "[2048 rows x 787 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>letter</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>774</th>\n",
       "      <th>775</th>\n",
       "      <th>776</th>\n",
       "      <th>777</th>\n",
       "      <th>778</th>\n",
       "      <th>779</th>\n",
       "      <th>780</th>\n",
       "      <th>781</th>\n",
       "      <th>782</th>\n",
       "      <th>783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2049</td>\n",
       "      <td>L</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2050</td>\n",
       "      <td>C</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2051</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2052</td>\n",
       "      <td>K</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2053</td>\n",
       "      <td>W</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20475</th>\n",
       "      <td>22524</td>\n",
       "      <td>P</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20476</th>\n",
       "      <td>22525</td>\n",
       "      <td>S</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20477</th>\n",
       "      <td>22526</td>\n",
       "      <td>B</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20478</th>\n",
       "      <td>22527</td>\n",
       "      <td>K</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20479</th>\n",
       "      <td>22528</td>\n",
       "      <td>S</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20480 rows × 786 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id letter  0  1  2  3  4  5  6  7  ...  774  775  776  777  778  \\\n",
       "0       2049      L  0  4  0  2  4  2  3  1  ...    2    0    4    2    2   \n",
       "1       2050      C  4  1  4  0  1  1  0  2  ...    0    3    2    4    2   \n",
       "2       2051      S  0  4  0  1  3  2  3  0  ...    1    3    2    0    3   \n",
       "3       2052      K  2  1  3  3  3  4  3  0  ...    3    0    3    2    4   \n",
       "4       2053      W  1  0  1  1  2  2  1  4  ...    4    3    1    4    0   \n",
       "...      ...    ... .. .. .. .. .. .. .. ..  ...  ...  ...  ...  ...  ...   \n",
       "20475  22524      P  1  2  1  1  0  0  0  2  ...    0    1    3    0    3   \n",
       "20476  22525      S  4  1  1  4  0  0  1  1  ...    1    3    1    0    0   \n",
       "20477  22526      B  4  2  1  3  2  1  3  0  ...    3    2    3    4    1   \n",
       "20478  22527      K  1  1  2  3  4  0  4  3  ...    2    0    0    4    3   \n",
       "20479  22528      S  2  1  0  3  0  3  3  1  ...    0    3    0    1    4   \n",
       "\n",
       "       779  780  781  782  783  \n",
       "0        4    3    4    1    4  \n",
       "1        4    2    2    1    2  \n",
       "2        2    3    0    1    4  \n",
       "3        1    0    4    4    4  \n",
       "4        2    1    2    3    4  \n",
       "...    ...  ...  ...  ...  ...  \n",
       "20475    0    4    3    1    4  \n",
       "20476    1    3    1    2    0  \n",
       "20477    0    3    3    1    1  \n",
       "20478    3    3    4    4    2  \n",
       "20479    2    0    2    2    0  \n",
       "\n",
       "[20480 rows x 786 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>digit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2049</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2050</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2051</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2052</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2053</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20475</th>\n",
       "      <td>22524</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20476</th>\n",
       "      <td>22525</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20477</th>\n",
       "      <td>22526</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20478</th>\n",
       "      <td>22527</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20479</th>\n",
       "      <td>22528</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20480 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  digit\n",
       "0       2049      0\n",
       "1       2050      0\n",
       "2       2051      0\n",
       "3       2052      0\n",
       "4       2053      0\n",
       "...      ...    ...\n",
       "20475  22524      0\n",
       "20476  22525      0\n",
       "20477  22526      0\n",
       "20478  22527      0\n",
       "20479  22528      0\n",
       "\n",
       "[20480 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(train, test, sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    233\n",
       "5    225\n",
       "6    212\n",
       "4    207\n",
       "3    205\n",
       "1    202\n",
       "9    197\n",
       "7    194\n",
       "0    191\n",
       "8    182\n",
       "Name: digit, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['digit'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2 = train.drop(['id', 'digit', 'letter'], axis=1)\n",
    "test2 = test.drop(['id', 'letter'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2 = train2.values\n",
    "test2 = test2.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1fb51dd64c0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train2[100].reshape(28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2 = train2.reshape(-1, 28, 28, 1)\n",
    "test2 = test2.reshape(-1, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2 = train2 / 255.0\n",
    "test2 = test2 / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "idg = ImageDataGenerator(height_shift_range=(-1,1), width_shift_range=(-1,1))\n",
    "idg2 = ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x720 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_data = train2[100].copy()\n",
    "sample_data.shape\n",
    "sample = expand_dims(sample_data, 0)\n",
    "sample_datagen = ImageDataGenerator(height_shift_range=(-1,1), width_shift_range=(-1,1))\n",
    "sample_generator = sample_datagen.flow(sample, batch_size=1)\n",
    "\n",
    "plt.figure(figsize=(16,10))\n",
    "\n",
    "for i in range(9):\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    sample_batch = sample_generator.next()\n",
    "    sample_image = sample_batch[0]\n",
    "    plt.imshow(sample_image.reshape(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=40, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-39-1c9dae048036>:58: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "Epoch 1/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 2.6374 - acc: 0.1598\n",
      "Epoch 00001: val_loss improved from inf to 6.22891, saving model to best_cvision.h5\n",
      "250/250 [==============================] - 32s 126ms/step - loss: 2.6374 - acc: 0.1598 - val_loss: 6.2289 - val_acc: 0.0962 - lr: 0.0020\n",
      "Epoch 2/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 2.0927 - acc: 0.2725\n",
      "Epoch 00002: val_loss did not improve from 6.22891\n",
      "250/250 [==============================] - 31s 123ms/step - loss: 2.0927 - acc: 0.2725 - val_loss: 7.7602 - val_acc: 0.0962 - lr: 0.0020\n",
      "Epoch 3/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 1.8080 - acc: 0.3632\n",
      "Epoch 00003: val_loss improved from 6.22891 to 2.07667, saving model to best_cvision.h5\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 1.8080 - acc: 0.3632 - val_loss: 2.0767 - val_acc: 0.4038 - lr: 0.0020\n",
      "Epoch 4/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 1.5174 - acc: 0.4810\n",
      "Epoch 00004: val_loss improved from 2.07667 to 0.96524, saving model to best_cvision.h5\n",
      "250/250 [==============================] - 32s 130ms/step - loss: 1.5174 - acc: 0.4810 - val_loss: 0.9652 - val_acc: 0.6154 - lr: 0.0020\n",
      "Epoch 5/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 1.3031 - acc: 0.5691\n",
      "Epoch 00005: val_loss improved from 0.96524 to 0.75084, saving model to best_cvision.h5\n",
      "250/250 [==============================] - 34s 137ms/step - loss: 1.3031 - acc: 0.5691 - val_loss: 0.7508 - val_acc: 0.7692 - lr: 0.0020\n",
      "Epoch 6/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 1.1402 - acc: 0.6268\n",
      "Epoch 00006: val_loss improved from 0.75084 to 0.69036, saving model to best_cvision.h5\n",
      "250/250 [==============================] - 33s 132ms/step - loss: 1.1402 - acc: 0.6268 - val_loss: 0.6904 - val_acc: 0.7885 - lr: 0.0020\n",
      "Epoch 7/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 1.0807 - acc: 0.6498\n",
      "Epoch 00007: val_loss improved from 0.69036 to 0.59557, saving model to best_cvision.h5\n",
      "250/250 [==============================] - 31s 124ms/step - loss: 1.0807 - acc: 0.6498 - val_loss: 0.5956 - val_acc: 0.8654 - lr: 0.0020\n",
      "Epoch 8/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 1.0182 - acc: 0.6789\n",
      "Epoch 00008: val_loss improved from 0.59557 to 0.54537, saving model to best_cvision.h5\n",
      "250/250 [==============================] - 32s 130ms/step - loss: 1.0182 - acc: 0.6789 - val_loss: 0.5454 - val_acc: 0.8462 - lr: 0.0020\n",
      "Epoch 9/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.9837 - acc: 0.6673\n",
      "Epoch 00009: val_loss improved from 0.54537 to 0.51954, saving model to best_cvision.h5\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 0.9837 - acc: 0.6673 - val_loss: 0.5195 - val_acc: 0.8269 - lr: 0.0020\n",
      "Epoch 10/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.8519 - acc: 0.7260\n",
      "Epoch 00010: val_loss did not improve from 0.51954\n",
      "250/250 [==============================] - 28s 113ms/step - loss: 0.8519 - acc: 0.7260 - val_loss: 0.5601 - val_acc: 0.7885 - lr: 0.0020\n",
      "Epoch 11/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.8473 - acc: 0.7290\n",
      "Epoch 00011: val_loss did not improve from 0.51954\n",
      "250/250 [==============================] - 29s 115ms/step - loss: 0.8473 - acc: 0.7290 - val_loss: 0.5790 - val_acc: 0.7885 - lr: 0.0020\n",
      "Epoch 12/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.8478 - acc: 0.7134\n",
      "Epoch 00012: val_loss improved from 0.51954 to 0.39215, saving model to best_cvision.h5\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 0.8478 - acc: 0.7134 - val_loss: 0.3922 - val_acc: 0.8654 - lr: 0.0020\n",
      "Epoch 13/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.8270 - acc: 0.7285\n",
      "Epoch 00013: val_loss did not improve from 0.39215\n",
      "250/250 [==============================] - 29s 115ms/step - loss: 0.8270 - acc: 0.7285 - val_loss: 0.4390 - val_acc: 0.8846 - lr: 0.0020\n",
      "Epoch 14/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.7614 - acc: 0.7515\n",
      "Epoch 00014: val_loss did not improve from 0.39215\n",
      "250/250 [==============================] - 29s 115ms/step - loss: 0.7614 - acc: 0.7515 - val_loss: 0.4736 - val_acc: 0.8269 - lr: 0.0020\n",
      "Epoch 15/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.7525 - acc: 0.7565\n",
      "Epoch 00015: val_loss did not improve from 0.39215\n",
      "250/250 [==============================] - 29s 114ms/step - loss: 0.7525 - acc: 0.7565 - val_loss: 0.4503 - val_acc: 0.8077 - lr: 0.0020\n",
      "Epoch 16/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.7418 - acc: 0.7685\n",
      "Epoch 00016: val_loss did not improve from 0.39215\n",
      "250/250 [==============================] - 29s 116ms/step - loss: 0.7418 - acc: 0.7685 - val_loss: 0.4738 - val_acc: 0.8846 - lr: 0.0020\n",
      "Epoch 17/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.7965 - acc: 0.7360\n",
      "Epoch 00017: val_loss did not improve from 0.39215\n",
      "250/250 [==============================] - 28s 113ms/step - loss: 0.7965 - acc: 0.7360 - val_loss: 0.4288 - val_acc: 0.8269 - lr: 0.0020\n",
      "Epoch 18/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.6664 - acc: 0.7831\n",
      "Epoch 00018: val_loss did not improve from 0.39215\n",
      "250/250 [==============================] - 29s 115ms/step - loss: 0.6664 - acc: 0.7831 - val_loss: 0.4056 - val_acc: 0.9038 - lr: 0.0020\n",
      "Epoch 19/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.6543 - acc: 0.7961\n",
      "Epoch 00019: val_loss improved from 0.39215 to 0.35800, saving model to best_cvision.h5\n",
      "250/250 [==============================] - 29s 118ms/step - loss: 0.6543 - acc: 0.7961 - val_loss: 0.3580 - val_acc: 0.8846 - lr: 0.0020\n",
      "Epoch 20/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.6158 - acc: 0.8036\n",
      "Epoch 00020: val_loss did not improve from 0.35800\n",
      "250/250 [==============================] - 29s 115ms/step - loss: 0.6158 - acc: 0.8036 - val_loss: 0.4267 - val_acc: 0.8269 - lr: 0.0020\n",
      "Epoch 21/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.6278 - acc: 0.7966\n",
      "Epoch 00021: val_loss did not improve from 0.35800\n",
      "250/250 [==============================] - 29s 115ms/step - loss: 0.6278 - acc: 0.7966 - val_loss: 0.5272 - val_acc: 0.8269 - lr: 0.0020\n",
      "Epoch 22/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.5704 - acc: 0.8181\n",
      "Epoch 00022: val_loss did not improve from 0.35800\n",
      "250/250 [==============================] - 29s 114ms/step - loss: 0.5704 - acc: 0.8181 - val_loss: 0.4483 - val_acc: 0.8654 - lr: 0.0020\n",
      "Epoch 23/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.5322 - acc: 0.8231\n",
      "Epoch 00023: val_loss did not improve from 0.35800\n",
      "250/250 [==============================] - 29s 114ms/step - loss: 0.5322 - acc: 0.8231 - val_loss: 0.5315 - val_acc: 0.8462 - lr: 0.0020\n",
      "Epoch 24/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.6028 - acc: 0.8161\n",
      "Epoch 00024: val_loss did not improve from 0.35800\n",
      "250/250 [==============================] - 29s 115ms/step - loss: 0.6028 - acc: 0.8161 - val_loss: 0.4029 - val_acc: 0.8654 - lr: 0.0020\n",
      "Epoch 25/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.5878 - acc: 0.8091\n",
      "Epoch 00025: val_loss did not improve from 0.35800\n",
      "250/250 [==============================] - 29s 115ms/step - loss: 0.5878 - acc: 0.8091 - val_loss: 0.3778 - val_acc: 0.8654 - lr: 0.0020\n",
      "Epoch 26/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.5228 - acc: 0.8332\n",
      "Epoch 00026: val_loss did not improve from 0.35800\n",
      "250/250 [==============================] - 29s 115ms/step - loss: 0.5228 - acc: 0.8332 - val_loss: 0.4151 - val_acc: 0.8654 - lr: 0.0020\n",
      "Epoch 27/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.6435 - acc: 0.7946\n",
      "Epoch 00027: val_loss did not improve from 0.35800\n",
      "250/250 [==============================] - 31s 123ms/step - loss: 0.6435 - acc: 0.7946 - val_loss: 0.3717 - val_acc: 0.8654 - lr: 0.0020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.5886 - acc: 0.8056\n",
      "Epoch 00028: val_loss did not improve from 0.35800\n",
      "250/250 [==============================] - 31s 125ms/step - loss: 0.5886 - acc: 0.8056 - val_loss: 0.3602 - val_acc: 0.8654 - lr: 0.0020\n",
      "Epoch 29/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.4851 - acc: 0.8422\n",
      "Epoch 00029: val_loss improved from 0.35800 to 0.16041, saving model to best_cvision.h5\n",
      "250/250 [==============================] - 31s 125ms/step - loss: 0.4851 - acc: 0.8422 - val_loss: 0.1604 - val_acc: 0.9038 - lr: 0.0020\n",
      "Epoch 30/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.5375 - acc: 0.8287\n",
      "Epoch 00030: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 30s 121ms/step - loss: 0.5375 - acc: 0.8287 - val_loss: 0.3635 - val_acc: 0.8846 - lr: 0.0020\n",
      "Epoch 31/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.4917 - acc: 0.8482\n",
      "Epoch 00031: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 31s 126ms/step - loss: 0.4917 - acc: 0.8482 - val_loss: 0.4636 - val_acc: 0.8462 - lr: 0.0020\n",
      "Epoch 32/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.5296 - acc: 0.8272\n",
      "Epoch 00032: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 32s 130ms/step - loss: 0.5296 - acc: 0.8272 - val_loss: 0.3945 - val_acc: 0.8654 - lr: 0.0020\n",
      "Epoch 33/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.5684 - acc: 0.8226\n",
      "Epoch 00033: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 32s 129ms/step - loss: 0.5684 - acc: 0.8226 - val_loss: 0.3375 - val_acc: 0.8846 - lr: 0.0020\n",
      "Epoch 34/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.6588 - acc: 0.7936\n",
      "Epoch 00034: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 0.6588 - acc: 0.7936 - val_loss: 0.4961 - val_acc: 0.8462 - lr: 0.0020\n",
      "Epoch 35/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.5225 - acc: 0.8397\n",
      "Epoch 00035: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 30s 122ms/step - loss: 0.5225 - acc: 0.8397 - val_loss: 0.3694 - val_acc: 0.8269 - lr: 0.0020\n",
      "Epoch 36/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.4667 - acc: 0.8587\n",
      "Epoch 00036: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 31s 125ms/step - loss: 0.4667 - acc: 0.8587 - val_loss: 0.3902 - val_acc: 0.8654 - lr: 0.0020\n",
      "Epoch 37/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.3818 - acc: 0.8773\n",
      "Epoch 00037: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 31s 123ms/step - loss: 0.3818 - acc: 0.8773 - val_loss: 0.3626 - val_acc: 0.8462 - lr: 0.0020\n",
      "Epoch 38/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.4481 - acc: 0.8602\n",
      "Epoch 00038: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 31s 124ms/step - loss: 0.4481 - acc: 0.8602 - val_loss: 0.3807 - val_acc: 0.8462 - lr: 0.0020\n",
      "Epoch 39/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.4032 - acc: 0.8783\n",
      "Epoch 00039: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 31s 124ms/step - loss: 0.4032 - acc: 0.8783 - val_loss: 0.5569 - val_acc: 0.8462 - lr: 0.0020\n",
      "Epoch 40/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.4228 - acc: 0.8642\n",
      "Epoch 00040: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 31s 123ms/step - loss: 0.4228 - acc: 0.8642 - val_loss: 0.2660 - val_acc: 0.9038 - lr: 0.0020\n",
      "Epoch 41/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.4228 - acc: 0.8712\n",
      "Epoch 00041: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 0.4228 - acc: 0.8712 - val_loss: 0.5202 - val_acc: 0.8846 - lr: 0.0020\n",
      "Epoch 42/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.4423 - acc: 0.8652\n",
      "Epoch 00042: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 0.4423 - acc: 0.8652 - val_loss: 0.4417 - val_acc: 0.8462 - lr: 0.0020\n",
      "Epoch 43/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.3996 - acc: 0.8682\n",
      "Epoch 00043: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 31s 124ms/step - loss: 0.3996 - acc: 0.8682 - val_loss: 0.3539 - val_acc: 0.8846 - lr: 0.0020\n",
      "Epoch 44/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.4335 - acc: 0.8587\n",
      "Epoch 00044: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 31s 123ms/step - loss: 0.4335 - acc: 0.8587 - val_loss: 0.4158 - val_acc: 0.8846 - lr: 0.0020\n",
      "Epoch 45/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.3855 - acc: 0.8737\n",
      "Epoch 00045: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 31s 125ms/step - loss: 0.3855 - acc: 0.8737 - val_loss: 0.3998 - val_acc: 0.8846 - lr: 0.0020\n",
      "Epoch 46/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.3976 - acc: 0.8692\n",
      "Epoch 00046: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 31s 126ms/step - loss: 0.3976 - acc: 0.8692 - val_loss: 0.4242 - val_acc: 0.8846 - lr: 0.0020\n",
      "Epoch 47/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.3849 - acc: 0.8803\n",
      "Epoch 00047: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 32s 130ms/step - loss: 0.3849 - acc: 0.8803 - val_loss: 0.4146 - val_acc: 0.8846 - lr: 0.0020\n",
      "Epoch 48/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.4395 - acc: 0.8652\n",
      "Epoch 00048: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 32s 129ms/step - loss: 0.4395 - acc: 0.8652 - val_loss: 0.3649 - val_acc: 0.9231 - lr: 0.0020\n",
      "Epoch 49/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.3814 - acc: 0.8848\n",
      "Epoch 00049: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 0.3814 - acc: 0.8848 - val_loss: 0.4146 - val_acc: 0.8846 - lr: 0.0020\n",
      "Epoch 50/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.3668 - acc: 0.8843\n",
      "Epoch 00050: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 31s 126ms/step - loss: 0.3668 - acc: 0.8843 - val_loss: 0.4333 - val_acc: 0.8462 - lr: 0.0020\n",
      "Epoch 51/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.3429 - acc: 0.8943\n",
      "Epoch 00051: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 31s 123ms/step - loss: 0.3429 - acc: 0.8943 - val_loss: 0.3763 - val_acc: 0.9038 - lr: 0.0020\n",
      "Epoch 52/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.3435 - acc: 0.8893\n",
      "Epoch 00052: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 31s 125ms/step - loss: 0.3435 - acc: 0.8893 - val_loss: 0.3291 - val_acc: 0.8654 - lr: 0.0020\n",
      "Epoch 53/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.3696 - acc: 0.8838\n",
      "Epoch 00053: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 0.3696 - acc: 0.8838 - val_loss: 0.4068 - val_acc: 0.8654 - lr: 0.0020\n",
      "Epoch 54/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.3837 - acc: 0.8788\n",
      "Epoch 00054: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 0.3837 - acc: 0.8788 - val_loss: 0.4146 - val_acc: 0.9038 - lr: 0.0020\n",
      "Epoch 55/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.3474 - acc: 0.8898\n",
      "Epoch 00055: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 0.3474 - acc: 0.8898 - val_loss: 0.4117 - val_acc: 0.8654 - lr: 0.0020\n",
      "Epoch 56/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.3743 - acc: 0.8803\n",
      "Epoch 00056: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 30s 119ms/step - loss: 0.3743 - acc: 0.8803 - val_loss: 0.3707 - val_acc: 0.8846 - lr: 0.0020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.3045 - acc: 0.9053\n",
      "Epoch 00057: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 0.3045 - acc: 0.9053 - val_loss: 0.3410 - val_acc: 0.8269 - lr: 0.0020\n",
      "Epoch 58/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.3518 - acc: 0.8958\n",
      "Epoch 00058: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 0.3518 - acc: 0.8958 - val_loss: 0.4115 - val_acc: 0.8654 - lr: 0.0020\n",
      "Epoch 59/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.3719 - acc: 0.8843\n",
      "Epoch 00059: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 29s 118ms/step - loss: 0.3719 - acc: 0.8843 - val_loss: 0.3500 - val_acc: 0.9038 - lr: 0.0020\n",
      "Epoch 60/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.3411 - acc: 0.8963\n",
      "Epoch 00060: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 0.3411 - acc: 0.8963 - val_loss: 0.4481 - val_acc: 0.8846 - lr: 0.0020\n",
      "Epoch 61/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.3150 - acc: 0.8958\n",
      "Epoch 00061: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 0.3150 - acc: 0.8958 - val_loss: 0.3991 - val_acc: 0.8846 - lr: 0.0020\n",
      "Epoch 62/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.3593 - acc: 0.8868\n",
      "Epoch 00062: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 29s 116ms/step - loss: 0.3593 - acc: 0.8868 - val_loss: 0.3472 - val_acc: 0.9038 - lr: 0.0020\n",
      "Epoch 63/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.2864 - acc: 0.9073\n",
      "Epoch 00063: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 0.2864 - acc: 0.9073 - val_loss: 0.3168 - val_acc: 0.9038 - lr: 0.0020\n",
      "Epoch 64/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.2703 - acc: 0.9173\n",
      "Epoch 00064: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 29s 118ms/step - loss: 0.2703 - acc: 0.9173 - val_loss: 0.2022 - val_acc: 0.9423 - lr: 0.0020\n",
      "Epoch 65/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.3200 - acc: 0.9018\n",
      "Epoch 00065: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 29s 116ms/step - loss: 0.3200 - acc: 0.9018 - val_loss: 0.3392 - val_acc: 0.9231 - lr: 0.0020\n",
      "Epoch 66/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.3018 - acc: 0.9068\n",
      "Epoch 00066: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 0.3018 - acc: 0.9068 - val_loss: 0.2963 - val_acc: 0.8846 - lr: 0.0020\n",
      "Epoch 67/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.2520 - acc: 0.9223\n",
      "Epoch 00067: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 0.2520 - acc: 0.9223 - val_loss: 0.4528 - val_acc: 0.9038 - lr: 0.0020\n",
      "Epoch 68/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.2481 - acc: 0.9213\n",
      "Epoch 00068: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 0.2481 - acc: 0.9213 - val_loss: 0.3336 - val_acc: 0.8846 - lr: 0.0020\n",
      "Epoch 69/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.3054 - acc: 0.9013\n",
      "Epoch 00069: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 0.3054 - acc: 0.9013 - val_loss: 0.4171 - val_acc: 0.8654 - lr: 0.0020\n",
      "Epoch 70/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.2990 - acc: 0.9063\n",
      "Epoch 00070: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 28s 114ms/step - loss: 0.2990 - acc: 0.9063 - val_loss: 0.4161 - val_acc: 0.8654 - lr: 0.0020\n",
      "Epoch 71/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.2987 - acc: 0.8998\n",
      "Epoch 00071: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 28s 113ms/step - loss: 0.2987 - acc: 0.8998 - val_loss: 0.4894 - val_acc: 0.8654 - lr: 0.0020\n",
      "Epoch 72/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.2635 - acc: 0.9118\n",
      "Epoch 00072: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 28s 111ms/step - loss: 0.2635 - acc: 0.9118 - val_loss: 0.3681 - val_acc: 0.9038 - lr: 0.0020\n",
      "Epoch 73/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.2446 - acc: 0.9259\n",
      "Epoch 00073: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 29s 114ms/step - loss: 0.2446 - acc: 0.9259 - val_loss: 0.3903 - val_acc: 0.8654 - lr: 0.0020\n",
      "Epoch 74/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.2484 - acc: 0.9208\n",
      "Epoch 00074: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 27s 110ms/step - loss: 0.2484 - acc: 0.9208 - val_loss: 0.3261 - val_acc: 0.8846 - lr: 0.0020\n",
      "Epoch 75/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.2521 - acc: 0.9228\n",
      "Epoch 00075: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 27s 109ms/step - loss: 0.2521 - acc: 0.9228 - val_loss: 0.3429 - val_acc: 0.8654 - lr: 0.0020\n",
      "Epoch 76/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.2305 - acc: 0.9289\n",
      "Epoch 00076: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 0.2305 - acc: 0.9289 - val_loss: 0.4599 - val_acc: 0.8462 - lr: 0.0020\n",
      "Epoch 77/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.2185 - acc: 0.9354\n",
      "Epoch 00077: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 0.2185 - acc: 0.9354 - val_loss: 0.4038 - val_acc: 0.8654 - lr: 0.0020\n",
      "Epoch 78/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.3030 - acc: 0.9023\n",
      "Epoch 00078: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 0.3030 - acc: 0.9023 - val_loss: 0.3949 - val_acc: 0.9038 - lr: 0.0020\n",
      "Epoch 79/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.2260 - acc: 0.9294\n",
      "Epoch 00079: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 0.2260 - acc: 0.9294 - val_loss: 0.3861 - val_acc: 0.8846 - lr: 0.0020\n",
      "Epoch 80/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.2123 - acc: 0.9349\n",
      "Epoch 00080: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 28s 110ms/step - loss: 0.2123 - acc: 0.9349 - val_loss: 0.5000 - val_acc: 0.8846 - lr: 0.0020\n",
      "Epoch 81/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.2692 - acc: 0.9193\n",
      "Epoch 00081: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 0.2692 - acc: 0.9193 - val_loss: 0.4208 - val_acc: 0.8846 - lr: 0.0020\n",
      "Epoch 82/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.2380 - acc: 0.9203\n",
      "Epoch 00082: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 28s 112ms/step - loss: 0.2380 - acc: 0.9203 - val_loss: 0.4178 - val_acc: 0.8654 - lr: 0.0020\n",
      "Epoch 83/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.2061 - acc: 0.9309\n",
      "Epoch 00083: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 27s 109ms/step - loss: 0.2061 - acc: 0.9309 - val_loss: 0.4005 - val_acc: 0.8846 - lr: 0.0020\n",
      "Epoch 84/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.2407 - acc: 0.9173\n",
      "Epoch 00084: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 28s 111ms/step - loss: 0.2407 - acc: 0.9173 - val_loss: 0.4740 - val_acc: 0.8846 - lr: 0.0020\n",
      "Epoch 85/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.2656 - acc: 0.9158\n",
      "Epoch 00085: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 28s 110ms/step - loss: 0.2656 - acc: 0.9158 - val_loss: 0.3688 - val_acc: 0.8846 - lr: 0.0020\n",
      "Epoch 86/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - ETA: 0s - loss: 0.2320 - acc: 0.9309\n",
      "Epoch 00086: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 0.2320 - acc: 0.9309 - val_loss: 0.4222 - val_acc: 0.8462 - lr: 0.0020\n",
      "Epoch 87/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.2064 - acc: 0.9394\n",
      "Epoch 00087: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 27s 107ms/step - loss: 0.2064 - acc: 0.9394 - val_loss: 0.2506 - val_acc: 0.9038 - lr: 0.0020\n",
      "Epoch 88/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.2276 - acc: 0.9304\n",
      "Epoch 00088: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 29s 115ms/step - loss: 0.2276 - acc: 0.9304 - val_loss: 0.3431 - val_acc: 0.9038 - lr: 0.0020\n",
      "Epoch 89/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.2023 - acc: 0.9329\n",
      "Epoch 00089: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 28s 114ms/step - loss: 0.2023 - acc: 0.9329 - val_loss: 0.3412 - val_acc: 0.9038 - lr: 0.0020\n",
      "Epoch 90/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.2544 - acc: 0.9148\n",
      "Epoch 00090: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 0.2544 - acc: 0.9148 - val_loss: 0.4614 - val_acc: 0.8846 - lr: 0.0020\n",
      "Epoch 91/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.2417 - acc: 0.9208\n",
      "Epoch 00091: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 0.2417 - acc: 0.9208 - val_loss: 0.4060 - val_acc: 0.9038 - lr: 0.0020\n",
      "Epoch 92/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.3060 - acc: 0.9058\n",
      "Epoch 00092: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 27s 110ms/step - loss: 0.3060 - acc: 0.9058 - val_loss: 0.3828 - val_acc: 0.8846 - lr: 0.0020\n",
      "Epoch 93/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.2220 - acc: 0.9314\n",
      "Epoch 00093: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 27s 109ms/step - loss: 0.2220 - acc: 0.9314 - val_loss: 0.3426 - val_acc: 0.9038 - lr: 0.0020\n",
      "Epoch 94/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.2012 - acc: 0.9364\n",
      "Epoch 00094: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 27s 109ms/step - loss: 0.2012 - acc: 0.9364 - val_loss: 0.3572 - val_acc: 0.9038 - lr: 0.0020\n",
      "Epoch 95/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.1813 - acc: 0.9419\n",
      "Epoch 00095: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 27s 110ms/step - loss: 0.1813 - acc: 0.9419 - val_loss: 0.3713 - val_acc: 0.9038 - lr: 0.0020\n",
      "Epoch 96/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.1530 - acc: 0.9484\n",
      "Epoch 00096: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 27s 110ms/step - loss: 0.1530 - acc: 0.9484 - val_loss: 0.4323 - val_acc: 0.9038 - lr: 0.0020\n",
      "Epoch 97/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.2045 - acc: 0.9374\n",
      "Epoch 00097: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 0.2045 - acc: 0.9374 - val_loss: 0.4274 - val_acc: 0.8846 - lr: 0.0020\n",
      "Epoch 98/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.2007 - acc: 0.9379\n",
      "Epoch 00098: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 28s 111ms/step - loss: 0.2007 - acc: 0.9379 - val_loss: 0.4045 - val_acc: 0.9038 - lr: 0.0020\n",
      "Epoch 99/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.2022 - acc: 0.9359\n",
      "Epoch 00099: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 28s 113ms/step - loss: 0.2022 - acc: 0.9359 - val_loss: 0.3142 - val_acc: 0.9038 - lr: 0.0020\n",
      "Epoch 100/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.2002 - acc: 0.9374\n",
      "Epoch 00100: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 28s 114ms/step - loss: 0.2002 - acc: 0.9374 - val_loss: 0.3394 - val_acc: 0.9038 - lr: 0.0020\n",
      "Epoch 101/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.1891 - acc: 0.9379\n",
      "Epoch 00101: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 27s 110ms/step - loss: 0.1891 - acc: 0.9379 - val_loss: 0.4044 - val_acc: 0.8462 - lr: 0.0020\n",
      "Epoch 102/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.2098 - acc: 0.9379\n",
      "Epoch 00102: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 27s 110ms/step - loss: 0.2098 - acc: 0.9379 - val_loss: 0.3337 - val_acc: 0.8846 - lr: 0.0020\n",
      "Epoch 103/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.1824 - acc: 0.9404\n",
      "Epoch 00103: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 28s 111ms/step - loss: 0.1824 - acc: 0.9404 - val_loss: 0.2993 - val_acc: 0.9423 - lr: 0.0020\n",
      "Epoch 104/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.1830 - acc: 0.9429\n",
      "Epoch 00104: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 0.1830 - acc: 0.9429 - val_loss: 0.1985 - val_acc: 0.9231 - lr: 0.0020\n",
      "Epoch 105/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.2029 - acc: 0.9404\n",
      "Epoch 00105: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 28s 111ms/step - loss: 0.2029 - acc: 0.9404 - val_loss: 0.2714 - val_acc: 0.9423 - lr: 0.0020\n",
      "Epoch 106/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.1641 - acc: 0.9434\n",
      "Epoch 00106: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 0.1641 - acc: 0.9434 - val_loss: 0.3068 - val_acc: 0.9231 - lr: 0.0020\n",
      "Epoch 107/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.1777 - acc: 0.9494\n",
      "Epoch 00107: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 29s 115ms/step - loss: 0.1777 - acc: 0.9494 - val_loss: 0.3249 - val_acc: 0.9038 - lr: 0.0020\n",
      "Epoch 108/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.1607 - acc: 0.9499\n",
      "Epoch 00108: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 28s 113ms/step - loss: 0.1607 - acc: 0.9499 - val_loss: 0.3559 - val_acc: 0.9038 - lr: 0.0020\n",
      "Epoch 109/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.1571 - acc: 0.9429\n",
      "Epoch 00109: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 28s 112ms/step - loss: 0.1571 - acc: 0.9429 - val_loss: 0.2161 - val_acc: 0.9231 - lr: 0.0020\n",
      "Epoch 110/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.1605 - acc: 0.9509\n",
      "Epoch 00110: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 28s 110ms/step - loss: 0.1605 - acc: 0.9509 - val_loss: 0.3693 - val_acc: 0.8846 - lr: 0.0020\n",
      "Epoch 111/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.2097 - acc: 0.9349\n",
      "Epoch 00111: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 28s 113ms/step - loss: 0.2097 - acc: 0.9349 - val_loss: 0.2754 - val_acc: 0.9231 - lr: 0.0020\n",
      "Epoch 112/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.1981 - acc: 0.9399\n",
      "Epoch 00112: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 29s 114ms/step - loss: 0.1981 - acc: 0.9399 - val_loss: 0.3185 - val_acc: 0.9231 - lr: 0.0020\n",
      "Epoch 113/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.1849 - acc: 0.9464\n",
      "Epoch 00113: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 29s 116ms/step - loss: 0.1849 - acc: 0.9464 - val_loss: 0.2917 - val_acc: 0.9038 - lr: 0.0020\n",
      "Epoch 114/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.1483 - acc: 0.9559\n",
      "Epoch 00114: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 29s 115ms/step - loss: 0.1483 - acc: 0.9559 - val_loss: 0.2959 - val_acc: 0.9038 - lr: 0.0020\n",
      "Epoch 115/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - ETA: 0s - loss: 0.2303 - acc: 0.9284\n",
      "Epoch 00115: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 28s 113ms/step - loss: 0.2303 - acc: 0.9284 - val_loss: 0.3152 - val_acc: 0.8846 - lr: 0.0020\n",
      "Epoch 116/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.2340 - acc: 0.9254\n",
      "Epoch 00116: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 28s 111ms/step - loss: 0.2340 - acc: 0.9254 - val_loss: 0.3433 - val_acc: 0.9038 - lr: 0.0020\n",
      "Epoch 117/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.1898 - acc: 0.9399\n",
      "Epoch 00117: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 28s 113ms/step - loss: 0.1898 - acc: 0.9399 - val_loss: 0.3693 - val_acc: 0.9038 - lr: 0.0020\n",
      "Epoch 118/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.1798 - acc: 0.9474\n",
      "Epoch 00118: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 28s 111ms/step - loss: 0.1798 - acc: 0.9474 - val_loss: 0.4108 - val_acc: 0.8846 - lr: 0.0020\n",
      "Epoch 119/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.1667 - acc: 0.9469\n",
      "Epoch 00119: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 28s 111ms/step - loss: 0.1667 - acc: 0.9469 - val_loss: 0.2919 - val_acc: 0.9038 - lr: 0.0020\n",
      "Epoch 120/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.1662 - acc: 0.9454\n",
      "Epoch 00120: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 28s 111ms/step - loss: 0.1662 - acc: 0.9454 - val_loss: 0.3334 - val_acc: 0.8846 - lr: 0.0020\n",
      "Epoch 121/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.1594 - acc: 0.9544\n",
      "Epoch 00121: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 28s 111ms/step - loss: 0.1594 - acc: 0.9544 - val_loss: 0.3906 - val_acc: 0.9038 - lr: 0.0020\n",
      "Epoch 122/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.1660 - acc: 0.9489\n",
      "Epoch 00122: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 29s 115ms/step - loss: 0.1660 - acc: 0.9489 - val_loss: 0.4501 - val_acc: 0.9038 - lr: 0.0020\n",
      "Epoch 123/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.1758 - acc: 0.9399\n",
      "Epoch 00123: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 29s 114ms/step - loss: 0.1758 - acc: 0.9399 - val_loss: 0.3749 - val_acc: 0.8846 - lr: 0.0020\n",
      "Epoch 124/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.1509 - acc: 0.9534\n",
      "Epoch 00124: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 28s 114ms/step - loss: 0.1509 - acc: 0.9534 - val_loss: 0.4052 - val_acc: 0.9038 - lr: 0.0020\n",
      "Epoch 125/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.1721 - acc: 0.9509\n",
      "Epoch 00125: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 28s 113ms/step - loss: 0.1721 - acc: 0.9509 - val_loss: 0.3085 - val_acc: 0.9038 - lr: 0.0020\n",
      "Epoch 126/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.1381 - acc: 0.9584\n",
      "Epoch 00126: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 29s 116ms/step - loss: 0.1381 - acc: 0.9584 - val_loss: 0.3810 - val_acc: 0.8654 - lr: 0.0020\n",
      "Epoch 127/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.1793 - acc: 0.9464\n",
      "Epoch 00127: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 28s 111ms/step - loss: 0.1793 - acc: 0.9464 - val_loss: 0.3673 - val_acc: 0.9231 - lr: 0.0020\n",
      "Epoch 128/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.1555 - acc: 0.9454\n",
      "Epoch 00128: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 29s 115ms/step - loss: 0.1555 - acc: 0.9454 - val_loss: 0.4594 - val_acc: 0.9038 - lr: 0.0020\n",
      "Epoch 129/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.1567 - acc: 0.9489\n",
      "Epoch 00129: val_loss did not improve from 0.16041\n",
      "\n",
      "Epoch 00129: ReduceLROnPlateau reducing learning rate to 0.0010000000474974513.\n",
      "250/250 [==============================] - 28s 112ms/step - loss: 0.1567 - acc: 0.9489 - val_loss: 0.4066 - val_acc: 0.9231 - lr: 0.0020\n",
      "Epoch 130/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.1602 - acc: 0.9539\n",
      "Epoch 00130: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 27s 109ms/step - loss: 0.1602 - acc: 0.9539 - val_loss: 0.4130 - val_acc: 0.9038 - lr: 0.0010\n",
      "Epoch 131/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.1527 - acc: 0.9524\n",
      "Epoch 00131: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 28s 111ms/step - loss: 0.1527 - acc: 0.9524 - val_loss: 0.4059 - val_acc: 0.9038 - lr: 0.0010\n",
      "Epoch 132/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.1325 - acc: 0.9604\n",
      "Epoch 00132: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 27s 109ms/step - loss: 0.1325 - acc: 0.9604 - val_loss: 0.3625 - val_acc: 0.9231 - lr: 0.0010\n",
      "Epoch 133/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.1292 - acc: 0.9584\n",
      "Epoch 00133: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 27s 110ms/step - loss: 0.1292 - acc: 0.9584 - val_loss: 0.4052 - val_acc: 0.9038 - lr: 0.0010\n",
      "Epoch 134/2000\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.1088 - acc: 0.9689\n",
      "Epoch 00134: val_loss did not improve from 0.16041\n",
      "250/250 [==============================] - 28s 112ms/step - loss: 0.1088 - acc: 0.9689 - val_loss: 0.3975 - val_acc: 0.8654 - lr: 0.0010\n",
      "Epoch 135/2000\n",
      "221/250 [=========================>....] - ETA: 4s - loss: 0.1170 - acc: 0.9637"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-1c9dae048036>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'sparse_categorical_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.002\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m     \u001b[0mlearning_history\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalid_generator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreLR\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;31m# predict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    322\u001b[0m               \u001b[1;34m'in a future version'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'after %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m               instructions)\n\u001b[1;32m--> 324\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[0;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'deprecated'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1463\u001b[0m     \"\"\"\n\u001b[0;32m   1464\u001b[0m     \u001b[0m_keras_api_gauge\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_cell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'fit_generator'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1465\u001b[1;33m     return self.fit(\n\u001b[0m\u001b[0;32m   1466\u001b[0m         \u001b[0mgenerator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1467\u001b[0m         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 848\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    849\u001b[0m               \u001b[1;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    850\u001b[0m               \u001b[1;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    579\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 580\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    581\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    609\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    610\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 611\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    612\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    613\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2418\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2420\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2422\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1659\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1660\u001b[0m     \"\"\"\n\u001b[1;32m-> 1661\u001b[1;33m     return self._call_flat(\n\u001b[0m\u001b[0;32m   1662\u001b[0m         (t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[0;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1743\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1744\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1745\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1746\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    591\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 593\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    594\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "reLR = ReduceLROnPlateau(patience=100,verbose=1,factor=0.5) #learning rate scheduler\n",
    "es = EarlyStopping(patience=160, verbose=1)\n",
    "\n",
    "val_loss_min = []\n",
    "result = 0\n",
    "nth = 0\n",
    "\n",
    "for train_index, valid_index in skf.split(train2,train['digit']) :\n",
    "    \n",
    "    mc = ModelCheckpoint('best_cvision.h5',save_best_only=True, verbose=1)\n",
    "    \n",
    "    x_train = train2[train_index]\n",
    "    x_valid = train2[valid_index]    \n",
    "    y_train = train['digit'][train_index]\n",
    "    y_valid = train['digit'][valid_index]\n",
    "    \n",
    "    train_generator = idg.flow(x_train,y_train,batch_size=8)\n",
    "    valid_generator = idg2.flow(x_valid,y_valid)\n",
    "    test_generator = idg2.flow(test2,shuffle=False)\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(16,(3,3),activation='relu',input_shape=(28,28,1),padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Conv2D(32,(3,3),activation='relu',padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(32,(5,5),activation='relu',padding='same')) \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(32,(5,5),activation='relu',padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(32,(5,5),activation='relu',padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((3,3)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Conv2D(64,(3,3),activation='relu',padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(64,(5,5),activation='relu',padding='same')) \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((3,3)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(128,activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(64,activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Dense(10,activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(lr=0.002,epsilon=None),metrics=['acc'])\n",
    "    \n",
    "    learning_history = model.fit_generator(train_generator,epochs=2000, validation_data=valid_generator, callbacks=[es,mc,reLR])\n",
    "    \n",
    "    # predict\n",
    "    model.load_weights('best_cvision.h5')\n",
    "    result += model.predict_generator(test_generator,verbose=True)/40\n",
    "    \n",
    "    # save val_loss\n",
    "    hist = pd.DataFrame(learning_history.history)\n",
    "    val_loss_min.append(hist['val_loss'].min())\n",
    "    \n",
    "    nth += 1\n",
    "    print(nth, '번째 학습을 완료했습니다.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(val_loss_min, np.mean(val_loss_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub['digit'] = result.argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('predict(95%).csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
